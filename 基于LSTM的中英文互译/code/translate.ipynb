{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LSTM.png\" alt=\"./\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = torch.randn([3,5])\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Cell\n",
    "class LSTMCell(nn.Module):\n",
    "\n",
    "    \" input_size  : Input data size \"\n",
    "    \" hidden_size : Hidden state size \"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        combine_size = input_size + hidden_size\n",
    "        # 定义输入门的线性层\n",
    "        self.in_gate = nn.Linear(combine_size, hidden_size)\n",
    "        # 定义遗忘门的线性层\n",
    "        self.forgot_gate = nn.Linear(combine_size, hidden_size)\n",
    "        # 定义备选细胞元的线性层\n",
    "        self.new_cell_state = nn.Linear(combine_size, hidden_size)\n",
    "        # 定义输出门的线性层\n",
    "        self.out_gate = nn.Linear(combine_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs, state=None):\n",
    "        '''\n",
    "        torch.cat是PyTorch中用于连接张量的函数，可以沿指定的维度将多个张量合并为一个张量\n",
    "        \n",
    "        向前传播\n",
    "        参数\n",
    "        ----\n",
    "        inputs ：torch.FloatTensor\n",
    "            输入数据，形状为(B, I)，其中B表示批量大小，I表示文字特征的长度（input_size）\n",
    "        state ：tuple(torch.FloatTensor, torch.FloatTensor)\n",
    "            (hidden state，cell state)，两个状态的形状都为(B, H)，其中H表示隐藏状态的长度（hidden_size）\n",
    "        返回\n",
    "        ----\n",
    "        hs ：torch.FloatTensor，hidden state，shape (B, H)\n",
    "        cs ：torch.FloatTensor，cell state，shape (B, H)\n",
    "        '''\n",
    "        B, _ = inputs.shape\n",
    "        if state is None:\n",
    "            state = self.init_state(B, inputs.device)\n",
    "        hs, cs = state\n",
    "        combined = torch.cat((inputs, hs), dim=1)           # (B, I + H)\n",
    "        # 输入门\n",
    "        ingate = F.sigmoid(self.in_gate(combined))          # (B,     H)\n",
    "        # 遗忘门\n",
    "        forgetgate = F.sigmoid(self.forgot_gate(combined))  # (B,     H)\n",
    "        # 输出门\n",
    "        outgate = F.sigmoid(self.out_gate(combined))        # (B,     H)\n",
    "        # 更新细胞状态\n",
    "        ncs = F.tanh(self.new_cell_state(combined))         # (B,     H)\n",
    "        cs = (forgetgate * cs) + (ingate * ncs)             # (B,     H)\n",
    "        # 更新隐藏状态\n",
    "        hs = outgate * F.tanh(cs)                           # (B,     H)\n",
    "        return hs, cs\n",
    "    \n",
    "    def init_state(self, B, device):\n",
    "        # 默认的隐藏状态和细胞状态全部都等于0\n",
    "        cs = torch.zeros((B, self.hidden_size), device=device) # Cell state\n",
    "        hs = torch.zeros((B, self.hidden_size), device=device) # Hidden state\n",
    "        return hs, cs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Network\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        '''\n",
    "        单层的长短期记忆网络（支持批量计算）\n",
    "        参数\n",
    "        ----\n",
    "        input_size ：int，输入数据的特征长度\n",
    "        hidden_size ：int，隐藏状态的特征长度\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = LSTMCell(self.input_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, inputs, state=None):\n",
    "        '''\n",
    "        向前传播\n",
    "        参数\n",
    "        ----\n",
    "        inputs ：torch.FloatTensor\n",
    "            输入数据的集合，形状为(B, T, C)，其中B表示批量大小，T表示文本长度，C表示文字特征的长度（input_size）\n",
    "        state ：tuple(torch.FloatTensor, torch.FloatTensor)\n",
    "            (初始的隐藏状态，初始的细胞状态)，两个状态的形状都为(B, H)，其中H表示隐藏状态的长度（hidden_size）\n",
    "        返回\n",
    "        ----\n",
    "        hidden ：torch.FloatTensor，所有隐藏状态的集合，形状为(B, T, H)\n",
    "        '''\n",
    "        re = []\n",
    "        B, T, C = inputs.shape\n",
    "        inputs = inputs.transpose(0, 1)  # (T, B, C)\n",
    "        for i in range(T):\n",
    "            state = self.lstm(inputs[i], state)\n",
    "            # 只记录隐藏状态，state[0]的形状为(B, H)\n",
    "            re.append(state[0])\n",
    "        result_tensor = torch.stack(re, dim=0)  # (T, B, H)\n",
    "        return result_tensor.transpose(0, 1)    # (B, T, H)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
